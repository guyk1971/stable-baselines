{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiprocessing_rl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/multiprocessing_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2",
        "colab_type": "text"
      },
      "source": [
        "# Stable Baselines, a fork of OpenAI Baselines - Easy Multiprocessing\n",
        "\n",
        "Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "[RL Baselines Zoo](https://github.com/araffin/rl-baselines-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines.readthedocs.io/](https://stable-baselines.readthedocs.io/)\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "```\n",
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines[mpi]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "503Gi2076F7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.8.0\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm",
        "colab_type": "text"
      },
      "source": [
        "## Import policy, RL agent, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import ACKTR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5WNF6G5gWZ1",
        "colab_type": "text"
      },
      "source": [
        "## Multiprocessing RL Training\n",
        "\n",
        "To multiprocess RL training, we will just have to wrap the Gym env into a SubprocVecEnv object, that will take care of synchronising the processes. The idea is that each process will run an indepedent instance of the Gym env.\n",
        "\n",
        "For that, we need an additional utility function, `make_env`, that will instantiate the environments and make sure they are different (using different random seed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgjfyOTPVxG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_env(env_id, rank, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "    \n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    set_global_seeds(seed)\n",
        "    return _init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPGIySi2g_RN",
        "colab_type": "text"
      },
      "source": [
        "The number of parallel process used is defined by the `num_cpu` variable.\n",
        "\n",
        "Because we use vectorized environment (SubprocVecEnv), the actions sent to the wrapped env must be an array (one action per process). Also, observations, rewards and dones are arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "num_cpu = 4  # Number of processes to use\n",
        "# Create the vectorized environment\n",
        "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
        "\n",
        "model = ACKTR(MlpPolicy, env, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl",
        "colab_type": "text"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63M8mSKR-6Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, num_steps=1000):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_steps: (int) number of timesteps to evaluate it\n",
        "    :return: (float) Mean reward\n",
        "    \"\"\"\n",
        "    episode_rewards = [[0.0] for _ in range(env.num_envs)]\n",
        "    obs = env.reset()\n",
        "    for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      actions, _states = model.predict(obs)\n",
        "      # here, action, rewards and dones are arrays\n",
        "      # because we are using vectorized env\n",
        "      obs, rewards, dones, info = env.step(actions)\n",
        "      \n",
        "      # Stats\n",
        "      for i in range(env.num_envs):\n",
        "          episode_rewards[i][-1] += rewards[i]\n",
        "          if dones[i]:\n",
        "              episode_rewards[i].append(0.0)\n",
        "\n",
        "    mean_rewards =  [0.0 for _ in range(env.num_envs)]\n",
        "    n_episodes = 0\n",
        "    for i in range(env.num_envs):\n",
        "        mean_rewards[i] = np.mean(episode_rewards[i])     \n",
        "        n_episodes += len(episode_rewards[i])   \n",
        "\n",
        "    # Compute mean reward\n",
        "    mean_reward = round(np.mean(mean_rewards), 1)\n",
        "    print(\"Mean reward:\", mean_reward, \"Num episodes:\", n_episodes)\n",
        "\n",
        "    return mean_reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95",
        "colab_type": "code",
        "outputId": "f72b75cf-27c3-4953-cdd9-ba2962257f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 22.5 Num episodes: 178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE",
        "colab_type": "text"
      },
      "source": [
        "## Multiprocess VS Single Process Training\n",
        "\n",
        "Here, we will compare time taken using one vs 4 processes, it should take ~30s in total."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "colab_type": "code",
        "outputId": "ec343743-7445-4350-f492-2c2450a81793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "n_timesteps = 25000\n",
        "\n",
        "# Multiprocessed RL Training\n",
        "start_time = time.time()\n",
        "model.learn(n_timesteps)\n",
        "total_time_multi = time.time() - start_time\n",
        "\n",
        "print(\"Took {:.2f}s for multiprocessed version - {:.2f} FPS\".format(total_time_multi, n_timesteps / total_time_multi))\n",
        "\n",
        "# Single Process RL Training\n",
        "single_process_model = ACKTR(MlpPolicy, DummyVecEnv([lambda: gym.make(env_id)]), verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "single_process_model.learn(n_timesteps)\n",
        "total_time_single = time.time() - start_time\n",
        "\n",
        "print(\"Took {:.2f}s for single process version - {:.2f} FPS\".format(total_time_single, n_timesteps / total_time_single))\n",
        "\n",
        "print(\"Multiprocessed training is {:.2f}x faster!\".format(total_time_single / total_time_multi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/acktr.py:287: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/acktr.py:288: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/kfac.py:968: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/kfac.py:909: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/kfac.py:616: The name tf.self_adjoint_eig is deprecated. Please use tf.linalg.eigh instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acktr/acktr.py:300: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Took 15.52s for multiprocessed version - 1610.79 FPS\n",
            "Took 29.27s for single process version - 854.20 FPS\n",
            "Multiprocessed training is 1.89x faster!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkWsoZ8emt0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
